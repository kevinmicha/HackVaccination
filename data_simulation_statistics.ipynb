{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ignore warning from pandas\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Setting the random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Number of tweets\n",
    "# n_tweets = 100\n",
    "\n",
    "# # Demographic data\n",
    "# ages = np.random.randint(15, 70, n_tweets)\n",
    "# genders = np.random.choice(['Male', 'Female', 'Non-binary'], n_tweets)\n",
    "# sentiments = np.random.choice(['Positive', 'Neutral', 'Negative'], n_tweets, p=[0.3, 0.4, 0.3])\n",
    "# languages = np.random.choice(['English', 'Spanish', 'French', 'German', 'Chinese'], n_tweets, p=[0.6, 0.15, 0.1, 0.1, 0.05])\n",
    "# locations = np.random.choice(['USA', 'UK', 'Canada', 'Australia', 'India', 'Germany'], n_tweets, p=[0.4, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
    "# # times = pd.date_range(start='2023-01-01', periods=n_tweets, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# # Generate random times using log-normal distribution (skewed towards earlier times)\n",
    "# time_offset_hours = np.random.lognormal(mean=3, sigma=1.5, size=n_tweets).astype(int)\n",
    "# if time_offset_hours.max() > 2:\n",
    "#     time_offset_hours = time_offset_hours % 2\n",
    "# time_offset_hours[-1] = 1\n",
    "# # Generate random minutes and seconds\n",
    "# time_offset_minutes = np.random.randint(0, 60, n_tweets)\n",
    "# time_offset_minutes[-1] = 59\n",
    "# time_offset_seconds = np.random.randint(0, 60, n_tweets)\n",
    "# time_offset_seconds[-1] = 59\n",
    "# # Generate base start time\n",
    "# base_time = pd.to_datetime('2023-01-01')\n",
    "# # Adding the time offset for hours, minutes, and seconds\n",
    "# times = base_time + pd.to_timedelta(time_offset_hours, unit='h') + pd.to_timedelta(time_offset_minutes, unit='m') + pd.to_timedelta(time_offset_seconds, unit='s')\n",
    "# # Engagement data\n",
    "# retweets = np.random.randint(0, 1000, n_tweets)\n",
    "# likes = np.random.randint(0, 5000, n_tweets)\n",
    "# comments = np.random.randint(0, 500, n_tweets)\n",
    "\n",
    "# # Generating random keywords for each tweet\n",
    "# keywords_pool = [\n",
    "#     'vaccine', 'health', 'COVID', 'immunity', 'science', 'safe', 'conspiracy', 'microchip', \n",
    "#     'hoax', 'flu', 'polio', 'measles', 'booster', 'mRNA', 'hospital', 'WHO', 'Pfizer',\n",
    "#     'Moderna', 'side effects', 'efficacy', 'trust', 'public health', 'community', 'trial', 'data'\n",
    "# ]\n",
    "# keywords = [', '.join(np.random.choice(keywords_pool, np.random.randint(2, 5), replace=False)) for _ in range(n_tweets)]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_tweets = pd.DataFrame({\n",
    "#     'Age': ages,\n",
    "#     'Gender': genders,\n",
    "#     'Sentiment': sentiments,\n",
    "#     'Language': languages,\n",
    "#     'Location': locations,\n",
    "#     'Time': times,\n",
    "#     'Retweets': retweets,\n",
    "#     'Likes': likes,\n",
    "#     'Comments': comments,\n",
    "#     'Keywords': keywords\n",
    "# })\n",
    "\n",
    "# # order the data by time\n",
    "# df_tweets = df_tweets.sort_values('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets.to_csv('df_simulated_tweets_2h.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_pool = [\n",
    "    'vaccine', 'health', 'COVID', 'immunity', 'science', 'safe', 'conspiracy', 'microchip', \n",
    "    'hoax', 'flu', 'polio', 'measles', 'booster', 'mRNA', 'hospital', 'WHO', 'Pfizer',\n",
    "    'Moderna', 'side effects', 'efficacy', 'trust', 'public health', 'community', 'trial', 'data'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newtweets_outbreak(old_df):\n",
    "    # Setting the random seed for reproducibility\n",
    "    # np.random.seed(42)\n",
    "    # get the last time of the old data\n",
    "    last_time = old_df['Time'].iloc[-1]\n",
    "    # simulate n new tweets, 20<=n<=50\n",
    "    n_tweets = 500\n",
    "    # Demographic data\n",
    "    ages = np.random.randint(15, 70, n_tweets)\n",
    "    genders = np.random.choice(['Male', 'Female', 'Non-binary'], n_tweets, p=[0.49, 0.5, 0.01])\n",
    "    sentiments = np.random.choice(['Positive', 'Neutral', 'Negative'], n_tweets, p=[0.2, 0.7, 0.1])\n",
    "    languages = np.random.choice(['English', 'Spanish', 'French', 'German', 'Chinese'], n_tweets, p=[0.6, 0.15, 0.1, 0.1, 0.05])\n",
    "    locations = np.random.choice(['USA', 'UK', 'Canada', 'Australia', 'India', 'Germany'], n_tweets, p=[0.4, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "    # Generate random minutes and seconds\n",
    "    time_offset_minutes = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    if time_offset_minutes.max() > 60:\n",
    "        time_offset_minutes = time_offset_minutes % 60\n",
    "    time_offset_minutes[-1] = 59\n",
    "    time_offset_seconds = np.random.randint(0, 60, n_tweets)\n",
    "    time_offset_seconds[-1] = 60\n",
    "    # Adding the time offset for hours, minutes, and seconds\n",
    "    times = last_time + pd.to_timedelta(time_offset_minutes, unit='m') + pd.to_timedelta(time_offset_seconds, unit='s')\n",
    "    # Engagement data\n",
    "    retweets = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    likes = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    comments = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "\n",
    "    keywords = [', '.join(np.random.choice(keywords_pool, np.random.randint(2, 5), replace=False)) for _ in range(n_tweets)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    new_tweets = pd.DataFrame({\n",
    "        'Age': ages,\n",
    "        'Gender': genders,\n",
    "        'Sentiment': sentiments,\n",
    "        'Language': languages,\n",
    "        'Location': locations,\n",
    "        'Time': times,\n",
    "        'Retweets': retweets,\n",
    "        'Likes': likes,\n",
    "        'Comments': comments,\n",
    "        'Keywords': keywords\n",
    "    })\n",
    "\n",
    "    # order the data by time\n",
    "    new_tweets = new_tweets.sort_values('Time')\n",
    "    # append the new tweets to the old data\n",
    "    new_df = pd.concat([old_df, new_tweets], ignore_index=True)\n",
    "    # only keep the tweets within the last 2 hours\n",
    "    new_last_time = new_df['Time'].iloc[-1]\n",
    "    new_df = new_df[new_df['Time'] >= new_last_time - pd.to_timedelta(2, unit='h')]\n",
    "    \n",
    "    return new_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newtweets_afterpost(old_df):\n",
    "    # get the last time of the old data\n",
    "    last_time = old_df['Time'].iloc[-1]\n",
    "    # simulate n new tweets, 20<=n<=50\n",
    "    n_tweets = 500\n",
    "    # Demographic data\n",
    "    ages = np.random.randint(15, 70, n_tweets)\n",
    "    genders = np.random.choice(['Male', 'Female', 'Non-binary'], n_tweets, p=[0.49, 0.5, 0.01])\n",
    "    sentiments = np.random.choice(['Positive', 'Neutral', 'Negative'], n_tweets, p=[0.5, 0.47, 0.03])\n",
    "    languages = np.random.choice(['English', 'Spanish', 'French', 'German', 'Chinese'], n_tweets, p=[0.6, 0.15, 0.1, 0.1, 0.05])\n",
    "    locations = np.random.choice(['USA', 'UK', 'Canada', 'Australia', 'India', 'Germany'], n_tweets, p=[0.4, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "    # Generate random minutes and seconds\n",
    "    time_offset_minutes = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    if time_offset_minutes.max() > 60:\n",
    "        time_offset_minutes = time_offset_minutes % 60\n",
    "    time_offset_seconds = np.random.randint(0, 60, n_tweets)\n",
    "    # Adding the time offset for hours, minutes, and seconds\n",
    "    times = last_time + pd.to_timedelta(time_offset_minutes, unit='m') + pd.to_timedelta(time_offset_seconds, unit='s')\n",
    "\n",
    "    # Engagement data\n",
    "    retweets = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    likes = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "    comments = np.random.lognormal(mean=3, sigma=0.5, size=n_tweets).astype(int)\n",
    "\n",
    "    keywords = [', '.join(np.random.choice(keywords_pool, np.random.randint(2, 5), replace=False)) for _ in range(n_tweets)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    new_tweets = pd.DataFrame({\n",
    "        'Age': ages,\n",
    "        'Gender': genders,\n",
    "        'Sentiment': sentiments,\n",
    "        'Language': languages,\n",
    "        'Location': locations,\n",
    "        'Time': times,\n",
    "        'Retweets': retweets,\n",
    "        'Likes': likes,\n",
    "        'Comments': comments,\n",
    "        'Keywords': keywords\n",
    "    })\n",
    "\n",
    "    # order the data by time\n",
    "    new_tweets = new_tweets.sort_values('Time')\n",
    "    # append the new tweets to the old data\n",
    "    new_df = pd.concat([old_df, new_tweets], ignore_index=True)\n",
    "    new_last_time = new_df['Time'].iloc[-1]\n",
    "    new_df = new_df[new_df['Time'] >= new_last_time - pd.to_timedelta(2, unit='h')]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newtweets_normal(old_df):\n",
    "    # get the last time of the old data\n",
    "    last_time = old_df['Time'].iloc[-1]\n",
    "    # simulate n new tweets, 20<=n<=50\n",
    "    n_tweets = np.random.randint(20, 100)\n",
    "    # Demographic data\n",
    "    ages = np.random.randint(15, 70, n_tweets)\n",
    "    genders = np.random.choice(['Male', 'Female', 'Non-binary'], n_tweets, p=[0.49, 0.5, 0.01])\n",
    "    sentiments = np.random.choice(['Positive', 'Neutral', 'Negative'], n_tweets, p=[0.05, 0.9, 0.05])\n",
    "    languages = np.random.choice(['English', 'Spanish', 'French', 'German', 'Chinese'], n_tweets, p=[0.6, 0.15, 0.1, 0.1, 0.05])\n",
    "    locations = np.random.choice(['USA', 'UK', 'Canada', 'Australia', 'India', 'Germany'], n_tweets, p=[0.4, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "    # Generate random minutes and seconds\n",
    "    time_offset_minutes = np.random.randint(0, 60, n_tweets)\n",
    "    if time_offset_minutes.max() > 60:\n",
    "        time_offset_minutes = time_offset_minutes % 60\n",
    "    time_offset_seconds = np.random.randint(0, 60, n_tweets)\n",
    "    # Adding the time offset for hours, minutes, and seconds\n",
    "    times = last_time + pd.to_timedelta(time_offset_minutes, unit='m') + pd.to_timedelta(time_offset_seconds, unit='s')\n",
    "\n",
    "    # Engagement data\n",
    "    retweets = np.random.randint(0, 1000, n_tweets)\n",
    "    likes = np.random.randint(0, 5000, n_tweets)\n",
    "    comments = np.random.randint(0, 500, n_tweets)\n",
    "\n",
    "    keywords = [', '.join(np.random.choice(keywords_pool, np.random.randint(2, 5), replace=False)) for _ in range(n_tweets)]\n",
    "\n",
    "    # Create DataFrame\n",
    "    new_tweets = pd.DataFrame({\n",
    "        'Age': ages,\n",
    "        'Gender': genders,\n",
    "        'Sentiment': sentiments,\n",
    "        'Language': languages,\n",
    "        'Location': locations,\n",
    "        'Time': times,\n",
    "        'Retweets': retweets,\n",
    "        'Likes': likes,\n",
    "        'Comments': comments,\n",
    "        'Keywords': keywords\n",
    "    })\n",
    "\n",
    "    # order the data by time\n",
    "    new_tweets = new_tweets.sort_values('Time')\n",
    "    # append the new tweets to the old data\n",
    "    new_df = pd.concat([old_df, new_tweets], ignore_index=True)\n",
    "    new_last_time = new_df['Time'].iloc[-1]\n",
    "    new_df = new_df[new_df['Time'] >= new_last_time - pd.to_timedelta(2, unit='h')]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Time' column to datetime type\n",
    "def n_sentiments(df):\n",
    "    df_tweets1 = df.copy()\n",
    "    df_tweets1['Time'] = pd.to_datetime(df_tweets1['Time'])\n",
    "    # Set 'Time' as the index\n",
    "    df_tweets1.set_index('Time', inplace=True)\n",
    "    # Resample the data to 24-hour intervals and get sentiment counts\n",
    "    sentiment_counts = df_tweets1.resample('1H')['Sentiment'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    return sentiment_counts\n",
    "\n",
    "def n_sentiments_new(old_sentiment, new_df):\n",
    "    # compute the sentiment counts for the new data and append to the old sentiments\n",
    "    new_sentiment = new_df['Sentiment'].value_counts()\n",
    "    # make this the same format as the old sentiment\n",
    "    new_sentiment = new_sentiment.reindex(old_sentiment.columns, fill_value=0)\n",
    "    new_sentiment = new_sentiment.to_frame().T\n",
    "    new_sentiment.index = [old_sentiment.index[-1] + pd.to_timedelta(1, unit='h')]\n",
    "    # append the new sentiment to the old sentiment\n",
    "    all_sentiment = pd.concat([old_sentiment, new_sentiment])\n",
    "    return all_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_engagements(df):\n",
    "    df_tweets1 = df.copy()\n",
    "    df_tweets1['Engegement'] = df_tweets1['Retweets'] + df_tweets1['Likes'] + df_tweets1['Comments']\n",
    "   \n",
    "    df_positive = df_tweets1[df_tweets1['Sentiment'] == 'Positive']\n",
    "    df_positive['Time'] = pd.to_datetime(df_positive['Time'])\n",
    "    df_positive.set_index('Time', inplace=True)\n",
    "    df_positive = df_positive.resample('1H')['Engegement'].sum()\n",
    "\n",
    "    df_negative = df_tweets1[df_tweets1['Sentiment'] == 'Negative']\n",
    "    df_negative['Time'] = pd.to_datetime(df_negative['Time'])\n",
    "    df_negative.set_index('Time', inplace=True)\n",
    "    df_negative = df_negative.resample('1H')['Engegement'].sum()\n",
    "\n",
    "    df_neutral = df_tweets1[df_tweets1['Sentiment'] == 'Neutral']\n",
    "    df_neutral['Time'] = pd.to_datetime(df_neutral['Time'])\n",
    "    df_neutral.set_index('Time', inplace=True)\n",
    "    # for a time period of 2 hours get cumulative engagement scores for each sentiment\n",
    "    df_neutral = df_neutral.resample('1H')['Engegement'].sum()\n",
    "    # return a dataframe where the columns are the sentiments and the rows are the cumulative engagement scores for each sentiment\n",
    "    return pd.DataFrame({'Positive': df_positive, 'Negative': df_negative, 'Neutral': df_neutral})\n",
    "\n",
    "def n_engagements_new(old_engagement, new_df):\n",
    "    # compute the engagement counts for the new data and append to the old engagements\n",
    "    new_df['Engegement'] = new_df['Retweets'] + new_df['Likes'] + new_df['Comments']\n",
    "    new_positive = new_df[new_df['Sentiment'] == 'Positive']\n",
    "    new_positive['Time'] = pd.to_datetime(new_positive['Time'])\n",
    "    new_positive.set_index('Time', inplace=True)\n",
    "    new_positive = new_positive['Engegement'].sum()\n",
    "\n",
    "    new_negative = new_df[new_df['Sentiment'] == 'Negative']\n",
    "    new_negative['Time'] = pd.to_datetime(new_negative['Time'])\n",
    "    new_negative.set_index('Time', inplace=True)\n",
    "    new_negative = new_negative['Engegement'].sum()\n",
    "\n",
    "    new_neutral = new_df[new_df['Sentiment'] == 'Neutral']\n",
    "    new_neutral['Time'] = pd.to_datetime(new_neutral['Time'])\n",
    "    new_neutral.set_index('Time', inplace=True)\n",
    "    new_neutral = new_neutral['Engegement'].sum()\n",
    "\n",
    "    # create a new row for the new engagements in the old engagements data\n",
    "    new_engagement = pd.DataFrame({'Positive': new_positive, 'Negative': new_negative, 'Neutral': new_neutral}, index=[old_engagement.index[-1] + pd.to_timedelta(1, unit='h')])\n",
    "    # append the new engagement to the old engagement\n",
    "    all_engagement = pd.concat([old_engagement, new_engagement])\n",
    "    return all_engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('df_simulated_tweets_2h.csv')\n",
    "old_sentiments = n_sentiments(df_tweets)\n",
    "old_engagements = n_engagements(df_tweets)\n",
    "df_tweets['Time'] = pd.to_datetime(df_tweets['Time'])\n",
    "# keep running this will generate new fake data and get the new sentiment and engagement statistics\n",
    "df_tweets_outbreak = add_newtweets_outbreak(df_tweets)\n",
    "n_sentiments_all = n_sentiments_new(old_sentiments, add_newtweets_outbreak(df_tweets_outbreak))\n",
    "n_engagements_all = n_engagements_new(old_engagements, add_newtweets_outbreak(df_tweets_outbreak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trigger1 as the number of negative sentiments exceeds 100\n",
    "def trigger1(n_sentiments_all):\n",
    "    if n_sentiments_all['Negative'].sum() > 100:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# define the trigger2 as the cumulative engagement score for negative sentiments exceeds 1000\n",
    "def trigger2(n_engagements_all):\n",
    "    if n_engagements_all['Negative'].sum() > 1000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# define the trigger3 as the change in ratio of negative sentiments between the last time period and the previous time period exceeds 0.5\n",
    "def trigger3(n_sentiments_all):\n",
    "    if n_sentiments_all['Negative'].iloc[-1] / n_sentiments_all['Negative'].iloc[-2] > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# define the trigger4 as the change in ratio of negative engagements between the last time period and the previous time period exceeds 0.5\n",
    "def trigger4(n_engagements_all):\n",
    "    if n_engagements_all['Negative'].iloc[-1] / n_engagements_all['Negative'].iloc[-2] > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#  define the overall trigger, if trigger 1 is true, then check if any two of the other triggers are true\n",
    "def overall_trigger(n_sentiments_all, n_engagements_all):\n",
    "    if trigger1(n_sentiments_all):\n",
    "        if trigger2(n_engagements_all) or trigger3(n_sentiments_all) or trigger4(n_engagements_all):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_trigger(n_sentiments_all, n_engagements_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is collected every 0.5 hour\n",
    "# only keep the data for 2 hours, e.g. if now is 2023-01-01 01:59:59, only keep the data from 2023-01-01 00:00:00 to 2023-01-01 01:59:59\n",
    "# get the sentiment counts updated every 0.5 hour using the data from the last 2 hours\n",
    "# get the engagement scores for each sentiment updated every 0.5 hour using the data from the last 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HOI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
